{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22711013",
   "metadata": {},
   "source": [
    "# Graded Lab: Agentic Workflows\n",
    "\n",
    "In this lab, you will build an agentic system that generates a short research report through planning, external tool usage, and feedback integration. Your workflow will involve:\n",
    "\n",
    "### Agents\n",
    "\n",
    "* **Planning Agent / Writer**: Creates an outline and coordinates tasks.\n",
    "* **Research Agent**: Gathers external information using tools like Arxiv, Tavily, and Wikipedia.\n",
    "* **Editor Agent**: Reflects on the report and provides suggestions for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc9f89",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to write your solution code or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the üíæ icon on the top left of the page and then click on the <span style=\"background-color: red; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4929dc",
   "metadata": {},
   "source": [
    "\n",
    "### Research Tools\n",
    "\n",
    "By importing `research_tools`, you gain access to several search utilities:\n",
    "\n",
    "- `research_tools.arxiv_search_tool(query)` ‚Üí search academic papers from **arXiv**  \n",
    "\n",
    "  *Example:* `research_tools.arxiv_search_tool(\"neural networks for climate modeling\")`\n",
    "\n",
    "- `research_tools.tavily_search_tool(query)` ‚Üí perform web searches with the **Tavily API**  \n",
    "\n",
    "  *Example:* `research_tools.tavily_search_tool(\"latest trends in sunglasses fashion\")`\n",
    "\n",
    "- `research_tools.wikipedia_search_tool(query)` ‚Üí retrieve summaries from **Wikipedia**  \n",
    "\n",
    "  *Example:* `research_tools.wikipedia_search_tool(\"Ensemble Kalman Filter\")`\n",
    "\n",
    "Run the cell below to make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9723175",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "\n",
    "# --- Standard library \n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "# --- Third-party ---\n",
    "from IPython.display import Markdown, display\n",
    "from aisuite import Client\n",
    "\n",
    "# --- Local / project ---\n",
    "import research_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf88f8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc8d9c",
   "metadata": {},
   "source": [
    "### Initialize client\n",
    "\n",
    "Create a shared client instance for upcoming calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42f388",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "CLIENT = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89313f5",
   "metadata": {},
   "source": [
    "## Exercise 1: planner_agent\n",
    "\n",
    "### Objective\n",
    "Correctly set up a call to a language model (LLM) to generate a research plan.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - Ensure `CLIENT.chat.completions.create` is correctly configured.\n",
    "   - Pass the `model` and `messages` parameters correctly:\n",
    "     - **Model**: Use `\"openai:o4-mini\"` by default.\n",
    "     - **Messages**: Set with `{\"role\": \"user\", \"content\": user_prompt}`.\n",
    "     - **Temperature**: Fixed at 1 for creative outputs.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The prompt is pre-defined and guides the LLM on task requirements.\n",
    "- Only return a formatted list of steps ‚Äî no extra text.\n",
    "\n",
    "Focus on the LLM call setup to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1add0d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: planner_agent\n",
    "\n",
    "def planner_agent(topic: str, model: str = \"openai:o4-mini\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a plan as a Python list of steps (strings) for a research workflow.\n",
    "\n",
    "    Args:\n",
    "        topic (str): Research topic to investigate.\n",
    "        model (str): Language model to use.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of executable step strings.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Build the user prompt\n",
    "    user_prompt = f\"\"\"\n",
    "    You are a planning agent responsible for organizing a research workflow with multiple intelligent agents.\n",
    "\n",
    "    üß† Available agents:\n",
    "    - A research agent who can search the web, Wikipedia, and arXiv.\n",
    "    - A writer agent who can draft research summaries.\n",
    "    - An editor agent who can reflect and revise the drafts.\n",
    "\n",
    "    üéØ Your job is to write a clear, step-by-step research plan **as a valid Python list**, where each step is a string.\n",
    "    Each step should be atomic, executable, and must rely only on the capabilities of the above agents.\n",
    "\n",
    "    üö´ DO NOT include irrelevant tasks like \"create CSV\", \"set up a repo\", \"install packages\", etc.\n",
    "    ‚úÖ DO include real research-related tasks (e.g., search, summarize, draft, revise).\n",
    "    ‚úÖ DO assume tool use is available.\n",
    "    ‚úÖ DO NOT include explanation text ‚Äî return ONLY the Python list.\n",
    "    ‚úÖ The final step should be to generate a Markdown document containing the complete research report.\n",
    "\n",
    "    Topic: \"{topic}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Add the user prompt to the messages list\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Call the LLM\n",
    "    response = CLIENT.chat.completions.create( \n",
    "        # Pass in the model\n",
    "        model=None,\n",
    "        # Define the messages. Remember this is meant to be a user prompt!\n",
    "        messages=None,\n",
    "        # Keep responses creative\n",
    "        temperature=1, \n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Extract message from response\n",
    "    steps_str = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Parse steps\n",
    "    steps = ast.literal_eval(steps_str)\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede74d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_planner_agent(planner_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14588d4c",
   "metadata": {},
   "source": [
    "## Exercise 2: research_agent\n",
    "\n",
    "### Objective\n",
    "Set up a call to a language model (LLM) to perform a research task using various tools.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**Focus Areas**:\n",
    "\n",
    "- **Creating a Custom Prompt**:\n",
    "  - **Define the Role**: Clearly specify the role, such as \"research assistant.\"\n",
    "  - **List Available Tools** (as strings inside the prompt, not the actual functions):\n",
    "    - Use `arxiv_tool` to find academic papers.\n",
    "    - Use `tavily_tool` for general web searches.\n",
    "    - Use `wikipedia_tool` for accessing encyclopedic knowledge.\n",
    "  - **Specify the Task**: Include a placeholder in your prompt for defining the specific task that needs to be accomplished.\n",
    "  - **Include Date Information**: Add a placeholder for the current date or time to provide context.\n",
    "\n",
    "- **Creating Messages Dict**:\n",
    "  - Ensure the `messages` are correctly set with `{\"role\": \"user\", \"content\": prompt}`.\n",
    "\n",
    "- **Creating Tools List**:\n",
    "  - Create a list of tools for use, such as `research_tools.arxiv_search_tool`, `research_tools.tavily_search_tool`, and `research_tools.wikipedia_search_tool`.\n",
    "\n",
    "- **Correctly Setting the Call to the LLM**:\n",
    "  - Pass the `model`, `messages`, and `tools` parameters accurately.\n",
    "  - Set `tool_choice` to `\"auto\"` for automatic tool selection.\n",
    "  - Limit interactions with `max_turns=6`.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The function provides pre-coded blocks where you need to replace placeholder values.\n",
    "- The approach allows the LLM to use tools dynamically based on the task.\n",
    "\n",
    "Focus on accurately setting the messages, tools, and LLM call parameters to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11f86e",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: research_agent\n",
    "\n",
    "def research_agent(task: str, model: str = \"openai:gpt-4o\", return_messages: bool = False):\n",
    "    \"\"\"\n",
    "    Executes a research task using tools via aisuite (no manual loop).\n",
    "    Returns either the assistant text, or (text, messages) if return_messages=True.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")  \n",
    "    print(\"üîç Research Agent\")                 \n",
    "    print(\"==================================\")\n",
    "\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a customizable prompt by defining the role (e.g., \"research assistant\"),\n",
    "    # listing tools (arxiv_tool, tavily_tool, wikipedia_tool) for various searches,\n",
    "    # specifying the task with a placeholder, and including a current_time placeholder.\n",
    "    prompt = None\n",
    "    \n",
    "    # Create the messages dict to pass to the LLM. Remember this is a user prompt!\n",
    "    messages = [{\"role\": None, \"content\": None}]\n",
    "\n",
    "    # Save all of your available tools in the tools list. These can be found in the research_tools module.\n",
    "    # You can identify each tool in your list like this: \n",
    "    # research_tools.<name_of_tool>, where <name_of_tool> is replaced with the function name of the tool.\n",
    "    tools = []\n",
    "    \n",
    "    # Call the model with tools enabled\n",
    "    response = CLIENT.chat.completions.create(  \n",
    "        # Set the model\n",
    "        model=None,\n",
    "        # Pass in the messages. You already defined this!\n",
    "        messages=None,\n",
    "        # Pass in the tools list. You already defined this!\n",
    "        tools=None,\n",
    "        # Set the LLM to automatically choose the tools\n",
    "        tool_choice=None,\n",
    "        # Set the max turns to 6\n",
    "        max_turns=None\n",
    "    )  \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"‚úÖ Output:\\n\", content)\n",
    "\n",
    "    \n",
    "    return (content, messages) if return_messages else content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9a0ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_research_agent(research_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313fa83",
   "metadata": {},
   "source": [
    "## Exercise 3: writer_agent\n",
    "\n",
    "### Objective\n",
    "Set up a call to a language model (LLM) for executing writing tasks like drafting, expanding, or summarizing text.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - **System Prompt**:\n",
    "     - Define `system_prompt` to assign the LLM the role of a writing agent focused on generating academic or technical content.\n",
    "   - **System and User Messages**:\n",
    "     - Create `system_msg` using `{\"role\": \"system\", \"content\": system_prompt}`.\n",
    "     - Create `user_msg` using `{\"role\": \"user\", \"content\": task}`.\n",
    "   - **Messages List**:\n",
    "     - Combine `system_msg` and `user_msg` into a `messages` list.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The function is designed to produce well-structured text by setting the correct prompts.\n",
    "- Temperature is set to 1.0 to allow for creative variance in the writing outputs.\n",
    "\n",
    "Ensure the system prompt and messages are defined properly to achieve a structured output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b34a6d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: writer_agent\n",
    "def writer_agent(task: str, model: str = \"openai:gpt-4o\") -> str: # @REPLACE def writer_agent(task: str, model: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Executes writing tasks, such as drafting, expanding, or summarizing text.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")\n",
    "    print(\"‚úçÔ∏è Writer Agent\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create the system prompt.\n",
    "    # This should assign the LLM the role of a writing agent specialized in generating well-structured academic or technical content\n",
    "    system_prompt = None\n",
    "\n",
    "    # Define the system msg by using the system_prompt and assigning the role of system\n",
    "    system_msg = None\n",
    "\n",
    "    # Define the user msg. In this case the user prompt should be the task passed to the function\n",
    "    user_msg = None\n",
    "\n",
    "    # Add both system and user messages to the messages list\n",
    "    messages = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93f22d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_writer_agent(writer_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918fa15",
   "metadata": {},
   "source": [
    "## Exercise 4: editor_agent\n",
    "\n",
    "### Objective\n",
    "Configure a call to a language model (LLM) to perform editorial tasks such as reflecting, critiquing, or revising drafts.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - **System Prompt**:\n",
    "     - Define `system_prompt` to assign the LLM the role of an editor agent whose task is to reflect on, critique, or improve drafts.\n",
    "   - **System and User Messages**:\n",
    "     - Create `system_msg` using `{\"role\": \"system\", \"content\": system_prompt}`.\n",
    "     - Create `user_msg` using `{\"role\": \"user\", \"content\": task}`.\n",
    "   - **Messages List**:\n",
    "     - Combine `system_msg` and `user_msg` into a `messages` list.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The editor agent is tailored for enhancing the quality of text by setting an appropriate role and task in the prompts.\n",
    "- Temperature is set to 0.7, balancing creativity and coherence in editorial outputs.\n",
    "\n",
    "Ensure the system prompt and messages are accurately set up to perform effective editorial tasks with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f4928",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: editor_agent\n",
    "def editor_agent(task: str, model: str = \"openai:gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Executes editorial tasks such as reflection, critique, or revision.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")\n",
    "    print(\"üß† Editor Agent\")\n",
    "    print(\"==================================\")\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create the system prompt.\n",
    "    # This should assign the LLM the role of an editor agent specialized in reflecting on, critiquing, or improving existing drafts.\n",
    "    system_prompt = None\n",
    "    \n",
    "    # Define the system msg by using the system_prompt and assigning the role of system\n",
    "    system_msg = None\n",
    "    \n",
    "    # Define the user msg. In this case the user prompt should be the task passed to the function\n",
    "    user_msg = None\n",
    "    \n",
    "    # Add both system and user messages to the messages list\n",
    "    messages = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        temperature=0.7 \n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096f973",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_editor_agent(editor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c98b1e",
   "metadata": {},
   "source": [
    "### üéØ The Executor Agent\n",
    "\n",
    "The `executor_agent` manages the workflow by executing each step of a given plan. It:\n",
    "\n",
    "1. Decides **which agent** (`research_agent`, `writer_agent`, or `editor_agent`) should handle the step.\n",
    "2. Builds context from the outputs of previous steps.\n",
    "3. Sends the enriched task to the selected agent.\n",
    "4. Collects and stores the results in a shared history.\n",
    "\n",
    "üëâ **Do not implement or modify this function.** It is already provided as the orchestration component of the multi-agent pipeline.\n",
    "\n",
    "Notice that `planner_agent` might return a long list of steps. Because of this, the maximum number of steps is set to a maximum of 4 to keep running time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d02e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "agent_registry = {\n",
    "    \"research_agent\": research_agent,\n",
    "    \"editor_agent\": editor_agent,\n",
    "    \"writer_agent\": writer_agent,\n",
    "}\n",
    "\n",
    "def clean_json_block(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the contents of a JSON block that may come wrapped with Markdown backticks.\n",
    "    \"\"\"\n",
    "    raw = raw.strip()\n",
    "    if raw.startswith(\"```\"):\n",
    "        raw = re.sub(r\"^```(?:json)?\\n?\", \"\", raw)\n",
    "        raw = re.sub(r\"\\n?```$\", \"\", raw)\n",
    "    return raw.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41493df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def executor_agent(topic, model: str = \"openai:gpt-4o\", limit_steps: bool = True):\n",
    "\n",
    "    plan_steps = planner_agent(topic)\n",
    "    max_steps = 4\n",
    "\n",
    "    if limit_steps:\n",
    "        plan_steps = plan_steps[:min(len(plan_steps), max_steps)]\n",
    "    \n",
    "    history = []\n",
    "\n",
    "    print(\"==================================\")\n",
    "    print(\"üéØ Editor Agent\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    for i, step in enumerate(plan_steps):\n",
    "\n",
    "        agent_decision_prompt = f\"\"\"\n",
    "        You are an execution manager for a multi-agent research team.\n",
    "\n",
    "        Given the following instruction, identify which agent should perform it and extract the clean task.\n",
    "\n",
    "        Return only a valid JSON object with two keys:\n",
    "        - \"agent\": one of [\"research_agent\", \"editor_agent\", \"writer_agent\"]\n",
    "        - \"task\": a string with the instruction that the agent should follow\n",
    "\n",
    "        Only respond with a valid JSON object. Do not include explanations or markdown formatting.\n",
    "\n",
    "        Instruction: \"{step}\"\n",
    "        \"\"\"\n",
    "        response = CLIENT.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": agent_decision_prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        raw_content = response.choices[0].message.content\n",
    "        cleaned_json = clean_json_block(raw_content)\n",
    "        agent_info = json.loads(cleaned_json)\n",
    "\n",
    "        agent_name = agent_info[\"agent\"]\n",
    "        task = agent_info[\"task\"]\n",
    "\n",
    "        context = \"\\n\".join([\n",
    "            f\"Step {j+1} executed by {a}:\\n{r}\" \n",
    "            for j, (s, a, r) in enumerate(history)\n",
    "        ])\n",
    "        enriched_task = f\"\"\"\n",
    "        You are {agent_name}.\n",
    "\n",
    "        Here is the context of what has been done so far:\n",
    "        {context}\n",
    "\n",
    "        Your next task is:\n",
    "        {task}\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nüõ†Ô∏è Executing with agent: `{agent_name}` on task: {task}\")\n",
    "\n",
    "        if agent_name in agent_registry:\n",
    "            output = agent_registry[agent_name](enriched_task)\n",
    "            history.append((step, agent_name, output))\n",
    "        else:\n",
    "            output = f\"‚ö†Ô∏è Unknown agent: {agent_name}\"\n",
    "            history.append((step, agent_name, output))\n",
    "\n",
    "        print(f\"‚úÖ Output:\\n{output}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f00f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# If you want to see the full workflow without limiting the number of steps. Set limit_steps to False\n",
    "# Keep in mind this could take more than 10 minutes to complete\n",
    "executor_history = executor_agent(\"The ensemble Kalman filter for time series forecasting\", limit_steps=True)\n",
    "\n",
    "md = executor_history[-1][-1].strip(\"`\")  \n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccebe7",
   "metadata": {},
   "source": [
    "## Check grading feedback\n",
    "\n",
    "If you have collapsed the right panel to have more screen space for your code, as shown below:\n",
    "\n",
    "<img src=\"./images/collapsed.png\" alt=\"Collapsed Image\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "You can click on the left-facing arrow button (highlighted in red) to view feedback for your submission after submitting it for grading. Once expanded, it should display like this:\n",
    "\n",
    "<img src=\"./images/expanded.png\" alt=\"Expanded Image\" width=\"800\" height=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "adlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
